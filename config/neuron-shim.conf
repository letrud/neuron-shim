# neuron-shim configuration
#
# Search order: ./neuron-shim.conf → /etc/neuron-shim.conf → defaults
#
# Backend selection:
#   auto   - prefer onnx (GPU) → tflite (CPU) → stub
#   onnx   - ONNX Runtime (NVIDIA CUDA/TensorRT, AMD MIGraphX, CPU)
#   tflite - TensorFlow Lite (CPU, optional GPU delegate)
#   stub   - no-op, returns zeros (for tracing/debugging)
backend = auto

# Model suffix appended to .dla paths:
#   .onnx   → model.dla.onnx   (for onnx backend)
#   .tflite → model.dla.tflite  (for tflite backend)
#   auto    → picks based on backend
suffix = auto

# Override model directory (optional).
# If set, models are loaded from this directory instead of the
# original path. The basename is preserved:
#   /usr/share/models/person_detect.dla → <model_dir>/person_detect.dla.onnx
# Leave empty or comment out to load from the original location.
# model_dir = /opt/neuron-shim/models
model_dir =

# Number of CPU threads for inference
threads = 4

# Force CPU-only execution (skip GPU providers)
# Useful for testing or when GPU drivers are broken
force_cpu = false

# Log level: 0=off 1=error 2=warn 3=info 4=debug
log_level = 3
